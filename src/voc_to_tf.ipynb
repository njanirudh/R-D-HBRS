{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Converts Pascal VOC data to TFRecords file format with Example protos.\n",
    "\n",
    "The raw Pascal VOC data set is expected to reside in JPEG files located in the\n",
    "directory 'JPEGImages'. Similarly, bounding box annotations are supposed to be\n",
    "stored in the 'Annotation directory'\n",
    "\n",
    "This TensorFlow script converts the training and evaluation data into\n",
    "a sharded data set consisting of 1024 and 128 TFRecord files, respectively.\n",
    "\n",
    "Each validation TFRecord file contains ~500 records. Each training TFREcord\n",
    "file contains ~1000 records. Each record within the TFRecord file is a\n",
    "serialized Example proto. The Example proto contains the following fields:\n",
    "\n",
    "    image/encoded: string containing JPEG encoded image in RGB colorspace\n",
    "    image/height: integer, image height in pixels\n",
    "    image/width: integer, image width in pixels\n",
    "    image/channels: integer, specifying the number of channels, always 3\n",
    "    image/format: string, specifying the format, always'JPEG'\n",
    "\n",
    "\n",
    "    image/object/bbox/xmin: list of float specifying the 0+ human annotated\n",
    "        bounding boxes\n",
    "    image/object/bbox/xmax: list of float specifying the 0+ human annotated\n",
    "        bounding boxes\n",
    "    image/object/bbox/ymin: list of float specifying the 0+ human annotated\n",
    "        bounding boxes\n",
    "    image/object/bbox/ymax: list of float specifying the 0+ human annotated\n",
    "        bounding boxes\n",
    "    image/object/bbox/label: list of integer specifying the classification index.\n",
    "    image/object/bbox/label_text: list of string descriptions.\n",
    "\n",
    "Note that the length of xmin is identical to the length of xmax, ymin and ymax\n",
    "for each example.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from datasets.dataset_utils import int64_feature, float_feature, bytes_feature\n",
    "# from datasets.pascalvoc_common import VOC_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original dataset organisation.\n",
    "DIRECTORY_ANNOTATIONS = 'Annotations/'\n",
    "DIRECTORY_IMAGES = 'JPEGImages/'\n",
    "\n",
    "# TFRecords convertion parameters.\n",
    "RANDOM_SEED = 4242\n",
    "SAMPLES_PER_FILES = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_MAPPING = {\n",
    "                  \"0\":\"door\",\n",
    "                  \"1\":\"handle\",\n",
    "                  \"2\":\"cabinet door\",\n",
    "                  \"3\":\"refrigerator door\"\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_image(directory, name):\n",
    "    \"\"\"Process a image and annotation file.\n",
    "\n",
    "    Args:\n",
    "      filename: string, path to an image file e.g., '/path/to/example.JPG'.\n",
    "      coder: instance of ImageCoder to provide TensorFlow image coding utils.\n",
    "    Returns:\n",
    "      image_buffer: string, JPEG encoding of RGB image.\n",
    "      height: integer, image height in pixels.\n",
    "      width: integer, image width in pixels.\n",
    "    \"\"\"\n",
    "    # Read the image file.\n",
    "    filename = directory + DIRECTORY_IMAGES + name + '.jpg'\n",
    "    image_data = tf.gfile.GFile(filename, 'rb').read()\n",
    "\n",
    "    # Read the XML annotation file.\n",
    "    filename = os.path.join(directory, DIRECTORY_ANNOTATIONS, name + '.xml')\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Image shape.\n",
    "    size = root.find('size')\n",
    "    shape = [int(size.find('height').text),\n",
    "             int(size.find('width').text),\n",
    "             int(size.find('depth').text)]\n",
    "    \n",
    "    # Find annotations.\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "    labels_text = []\n",
    "    difficult = []\n",
    "    truncated = []\n",
    "    for obj in root.findall('object'):\n",
    "        label = obj.find('name').text\n",
    "        labels.append(int(VOC_LABELS[label][0]))\n",
    "        labels_text.append(label.encode('ascii'))\n",
    "\n",
    "        if obj.find('difficult'):\n",
    "            difficult.append(int(obj.find('difficult').text))\n",
    "        else:\n",
    "            difficult.append(0)\n",
    "        if obj.find('truncated'):\n",
    "            truncated.append(int(obj.find('truncated').text))\n",
    "        else:\n",
    "            truncated.append(0)\n",
    "\n",
    "        bbox = obj.find('bndbox')\n",
    "        bboxes.append((float(bbox.find('ymin').text) / shape[0],\n",
    "                       float(bbox.find('xmin').text) / shape[1],\n",
    "                       float(bbox.find('ymax').text) / shape[0],\n",
    "                       float(bbox.find('xmax').text) / shape[1]\n",
    "                       ))\n",
    "    return image_data, shape, bboxes, labels, labels_text, difficult, truncated\n",
    "\n",
    "\n",
    "def _convert_to_example(image_data, labels, labels_text, bboxes, shape,\n",
    "                        difficult, truncated):\n",
    "    \"\"\"Build an Example proto for an image example.\n",
    "\n",
    "    Args:\n",
    "      image_data: string, JPEG encoding of RGB image;\n",
    "      labels: list of integers, identifier for the ground truth;\n",
    "      labels_text: list of strings, human-readable labels;\n",
    "      bboxes: list of bounding boxes; each box is a list of integers;\n",
    "          specifying [xmin, ymin, xmax, ymax]. All boxes are assumed to belong\n",
    "          to the same label as the image label.\n",
    "      shape: 3 integers, image shapes in pixels.\n",
    "    Returns:\n",
    "      Example proto\n",
    "    \"\"\"\n",
    "    xmin = []\n",
    "    ymin = []\n",
    "    xmax = []\n",
    "    ymax = []\n",
    "    for b in bboxes:\n",
    "        assert len(b) == 4\n",
    "        # pylint: disable=expression-not-assigned\n",
    "        [l.append(point) for l, point in zip([ymin, xmin, ymax, xmax], b)]\n",
    "        # pylint: enable=expression-not-assigned\n",
    "\n",
    "    image_format = b'JPEG'\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'image/height': int64_feature(shape[0]),\n",
    "            'image/width': int64_feature(shape[1]),\n",
    "            'image/channels': int64_feature(shape[2]),\n",
    "            'image/shape': int64_feature(shape),\n",
    "            'image/object/bbox/xmin': float_feature(xmin),\n",
    "            'image/object/bbox/xmax': float_feature(xmax),\n",
    "            'image/object/bbox/ymin': float_feature(ymin),\n",
    "            'image/object/bbox/ymax': float_feature(ymax),\n",
    "            'image/object/bbox/label': int64_feature(labels),\n",
    "            'image/object/bbox/label_text': bytes_feature(labels_text),\n",
    "            'image/object/bbox/difficult': int64_feature(difficult),\n",
    "            'image/object/bbox/truncated': int64_feature(truncated),\n",
    "            'image/format': bytes_feature(image_format),\n",
    "            'image/encoded': bytes_feature(image_data)}))\n",
    "    return example\n",
    "\n",
    "\n",
    "def _add_to_tfrecord(dataset_dir, name, tfrecord_writer):\n",
    "    \"\"\"Loads data from image and annotations files and add them to a TFRecord.\n",
    "\n",
    "    Args:\n",
    "      dataset_dir: Dataset directory;\n",
    "      name: Image name to add to the TFRecord;\n",
    "      tfrecord_writer: The TFRecord writer to use for writing.\n",
    "    \"\"\"\n",
    "    image_data, shape, bboxes, labels, labels_text, difficult, truncated = \\\n",
    "        _process_image(dataset_dir, name)\n",
    "    example = _convert_to_example(image_data, labels, labels_text,\n",
    "                                  bboxes, shape, difficult, truncated)\n",
    "    tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "\n",
    "def _get_output_filename(output_dir, name, idx):\n",
    "    return '%s/%s_%03d.tfrecord' % (output_dir, name, idx)\n",
    "\n",
    "\n",
    "def run(dataset_dir, output_dir, name='voc_train', shuffling=False):\n",
    "    \"\"\"Runs the conversion operation.\n",
    "\n",
    "    Args:\n",
    "      dataset_dir: The dataset directory where the dataset is stored.\n",
    "      output_dir: Output directory.\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(dataset_dir):\n",
    "        tf.gfile.MakeDirs(dataset_dir)\n",
    "\n",
    "    # Dataset filenames, and shuffling.\n",
    "    path = os.path.join(dataset_dir, DIRECTORY_ANNOTATIONS)\n",
    "    filenames = sorted(os.listdir(path))\n",
    "    if shuffling:\n",
    "        random.seed(RANDOM_SEED)\n",
    "        random.shuffle(filenames)\n",
    "\n",
    "    # Process dataset files.\n",
    "    i = 0\n",
    "    fidx = 0\n",
    "    while i < len(filenames):\n",
    "        # Open new TFRecord file.\n",
    "        tf_filename = _get_output_filename(output_dir, name, fidx)\n",
    "        with tf.python_io.TFRecordWriter(tf_filename) as tfrecord_writer:\n",
    "            j = 0\n",
    "            while i < len(filenames) and j < SAMPLES_PER_FILES:\n",
    "                sys.stdout.write('\\r>> Converting image %d/%d' % (i+1, len(filenames)))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                filename = filenames[i]\n",
    "                img_name = filename[:-4]\n",
    "                _add_to_tfrecord(dataset_dir, img_name, tfrecord_writer)\n",
    "                i += 1\n",
    "                j += 1\n",
    "            fidx += 1\n",
    "\n",
    "    # Finally, write the labels file:\n",
    "    # labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n",
    "    # dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n",
    "    print('\\nFinished converting the Pascal VOC dataset!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 1/1170"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "[] has type list, but expected one of: bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-20f541d9d897>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m run(\"/home/nj/Desktop/RnD/Dataset/DoorDetect-Dataset/ds/\",\n\u001b[0;32m----> 2\u001b[0;31m    \"/home/nj/Desktop/RnD/Dataset/DoorDetect-Dataset/ds/\")\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-760fbd9d9e2d>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(dataset_dir, output_dir, name, shuffling)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0m_add_to_tfrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfrecord_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-760fbd9d9e2d>\u001b[0m in \u001b[0;36m_add_to_tfrecord\u001b[0;34m(dataset_dir, name, tfrecord_writer)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0m_process_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     example = _convert_to_example(image_data, labels, labels_text,\n\u001b[0;32m--> 110\u001b[0;31m                                   bboxes, shape, difficult, truncated)\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mtfrecord_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-760fbd9d9e2d>\u001b[0m in \u001b[0;36m_convert_to_example\u001b[0;34m(image_data, labels, labels_text, bboxes, shape, difficult, truncated)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;34m'image/object/bbox/ymax'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;34m'image/object/bbox/label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint64_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;34m'image/object/bbox/label_text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0;34m'image/object/bbox/difficult'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint64_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifficult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;34m'image/object/bbox/truncated'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint64_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/slim-0.1-py3.7.egg/datasets/dataset_utils.py\u001b[0m in \u001b[0;36mbytes_feature\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mTF\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: [] has type list, but expected one of: bytes"
     ]
    }
   ],
   "source": [
    "run(\"/home/nj/Desktop/RnD/Dataset/DoorDetect-Dataset/ds/\",\n",
    "   \"/home/nj/Desktop/RnD/Dataset/DoorDetect-Dataset/ds/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
