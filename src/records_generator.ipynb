{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import io\n",
    "import glob\n",
    "import hashlib\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def int64_list_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def bytes_list_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def float_list_feature(value):\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_record(image_path, label_path, class_names):\n",
    "    #load image\n",
    "    with tf.gfile.GFile(image_path, 'rb') as fid:\n",
    "        encoded_img = fid.read()\n",
    "\n",
    "    encoded_img_io = io.BytesIO(encoded_img)\n",
    "    image = Image.open(encoded_img_io)\n",
    "    key = hashlib.sha256(encoded_img).hexdigest()\n",
    "\n",
    "    width, height = image.size\n",
    "\n",
    "    #read annotation\n",
    "    with open(label_path, 'r') as li:\n",
    "        annotations = li.readlines()\n",
    "\n",
    "    #convert annotations to tensorflow format\n",
    "    xmin = []\n",
    "    ymin = []\n",
    "    xmax = []\n",
    "    ymax = []\n",
    "    classes = []\n",
    "    classes_text = []\n",
    "    truncated = []\n",
    "    poses = []\n",
    "    difficult_obj = []\n",
    "    for a in annotations:\n",
    "        c_id, x, y, w, h = a.strip().split(' ')\n",
    "        c_id=int(c_id)\n",
    "        x=float(x)\n",
    "        y=float(y)\n",
    "        w=float(w)\n",
    "        h=float(h)\n",
    "\n",
    "        xmin.append(float(x - (w / 2)))\n",
    "        ymin.append(float(y - (h / 2)))\n",
    "        xmax.append(float(x + (w / 2)))\n",
    "        ymax.append(float(y + (h / 2)))\n",
    "        #class 0 is for background?\n",
    "        classes.append(c_id+1)\n",
    "        classes_text.append(class_names[c_id].encode('utf8'))\n",
    "        #????????????????????????????????????\n",
    "        truncated.append(0)\n",
    "        poses.append(''.encode('utf8'))\n",
    "        difficult_obj.append(int(False))\n",
    "\n",
    "    #create tfrecords\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': int64_feature(height),\n",
    "        'image/width': int64_feature(width),\n",
    "        'image/filename': bytes_feature(image_path.encode('utf8')),\n",
    "        'image/source_id': bytes_feature(image_path.encode('utf8')),\n",
    "        'image/key/sha256': bytes_feature(key.encode('utf8')),\n",
    "        'image/encoded': bytes_feature(encoded_img),\n",
    "        'image/format': bytes_feature(image_path[-3:].encode('utf8')),\n",
    "        'image/object/bbox/xmin': float_list_feature(xmin),\n",
    "        'image/object/bbox/xmax': float_list_feature(xmax),\n",
    "        'image/object/bbox/ymin': float_list_feature(ymin),\n",
    "        'image/object/bbox/ymax': float_list_feature(ymax),\n",
    "        'image/object/class/text': bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': int64_list_feature(classes),\n",
    "        'image/object/difficult': int64_list_feature(difficult_obj),\n",
    "        'image/object/truncated': int64_list_feature(truncated),\n",
    "        'image/object/view': bytes_list_feature(poses),\n",
    "    }))\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yolo_tf(train_file,tf_record_out,label_name_out,class_to_name):\n",
    "    try:\n",
    "        file_list=[]\n",
    "        with open(train_file) as f_in:\n",
    "            file_list=f_in.readlines()\n",
    "        \n",
    "        class_names=[]\n",
    "        with open(class_to_name) as f_in:\n",
    "            class_names=f_in.readlines()\n",
    "        class_names=[c.strip() for c in class_names if len(c)>0]\n",
    "    except Exception as e:\n",
    "        print('Caught Exception: {}'.format(e))\n",
    "        print('Shutting Down')\n",
    "        exit()\n",
    "    \n",
    "    label_list = [f.strip().replace('images','labels')[:-4]+'.txt' for f in file_list]\n",
    "    \n",
    "\n",
    "    #create tensorflow writer to write the final tfrecord\n",
    "    writer = tf.python_io.TFRecordWriter(tf_record_out)\n",
    "\n",
    "    for idx,(f,l) in enumerate(zip(file_list,label_list)):\n",
    "        #get a single tfrecord describing the image and annotations\n",
    "        example = get_tf_record(f.strip(), l, class_names)\n",
    "\n",
    "        #write tfrecord\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "        print('{}/{}'.format(idx, len(file_list)),end='\\r')\n",
    "\n",
    "    print('\\nTFRecord saved, creating label_name.pbtxt')\n",
    "    with open(label_name_out, 'w+') as f_out:\n",
    "        proto_string=\"\\nitem{{\\n\\tid: {}\\n\\tname: '{}' \\n }}\\n\"\n",
    "        for i,c in enumerate(class_names):\n",
    "            f_out.write(proto_string.format(i+1,c))\n",
    "    \n",
    "    print('Conversion Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/home/nj/HBRS/R&D/Dataset/DoorDetect-Dataset/dataset\"\n",
    "TF_FILE_PATH = \"/home/nj/HBRS/R&D/Dataset/DoorDetect-Dataset/tf_data.tfrecord\"\n",
    "\n",
    "img_file_list = glob.glob(os.path.join(DATASET_PATH,\"*.jpg\"), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {0:\"door\",\n",
    "              1:\"handle\",\n",
    "              2:\"cabinet door\",\n",
    "              3:\"refrigerator door\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer =  tf.io.TFRecordWriter(TF_FILE_PATH)\n",
    "\n",
    "for i,img in enumerate(img_file_list) :\n",
    "    name = Path(img).stem\n",
    "    img_path = os.path.join(DATASET_PATH , name + \".jpg\")\n",
    "    annotation_path = os.path.join(DATASET_PATH ,name  + \".txt\")\n",
    "    \n",
    "    tf_record = get_tf_record(img_path,annotation_path,class_names)\n",
    "    #sprint(tf_record.SerializeToString())\n",
    "#     writer.write(tf_record.SerializeToString())\n",
    "    \n",
    "#     print(i,\" \",name,\" written to tf record.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_CONFIG_PATH = \"/home/nj/HBRS/R&D/Github/models/research/object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config\"\n",
    "MODEL_DIR=\"/home/nj/HBRS/R&D/Dataset/DoorDetect-Dataset/Model/\"\n",
    "NUM_TRAIN_STEPS=50000\n",
    "SAMPLE_1_OF_N_EVAL_EXAMPLES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python object_detection/model_main.py \\\n",
    "    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\n",
    "    --model_dir=${MODEL_DIR} \\\n",
    "    --num_train_steps=${NUM_TRAIN_STEPS} \\\n",
    "    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \\\n",
    "    --alsologtostderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python /home/nj/HBRS/R&D/Github/models/research/object_detection/model_main.py --pipeline_config_path=/home/nj/HBRS/R&D/Github/models/research/object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config --model_dir=/home/nj/HBRS/R&D/Dataset/DoorDetect-Dataset/Model/ --num_train_steps=50000 --sample_1_of_n_eval_examples=1 --alsologtostderr\n"
     ]
    }
   ],
   "source": [
    "cmd = ['python','/home/nj/HBRS/R&D/Github/models/research/object_detection/model_main.py',\n",
    "        '--pipeline_config_path='+PIPELINE_CONFIG_PATH,\n",
    "        '--model_dir='+MODEL_DIR,\n",
    "        '--num_train_steps='+str(NUM_TRAIN_STEPS),\n",
    "        '--sample_1_of_n_eval_examples='+str(SAMPLE_1_OF_N_EVAL_EXAMPLES),\n",
    "        '--alsologtostderr']\n",
    "print(' '.join(cmd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = subprocess.Popen(cmd, stdout=subprocess.PIPE, bufsize=1)\n",
    "for line in iter(p.stdout.readline, b''):\n",
    "    print(line),\n",
    "p.stdout.close()\n",
    "p.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://towardsdatascience.com/working-with-tfrecords-and-tf-train-example-36d111b3ff4d\n",
    "2. https://github.com/AlessioTonioni/tf-objdetector/blob/master/yolo_tf_converter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python \"/home/nj/HBRS/R&D/Github/models/research/object_detection/model_main.py\" --pipeline_config_path=\"/home/nj/HBRS/R&D/Github/models/research/object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config\" --model_dir=/home/nj/HBRS/R&D/Dataset/DoorDetect-Dataset/Model/ --num_train_steps=50000 --sample_1_of_n_eval_examples=1 --alsologtostderr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
